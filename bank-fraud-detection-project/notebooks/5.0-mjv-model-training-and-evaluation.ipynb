{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f4da02",
   "metadata": {},
   "source": [
    "# 5.0 - Model Training, Evaluation, and Business Simulation\n",
    "\n",
    "_by Michael Joshua Vargas_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d454bf",
   "metadata": {},
   "source": [
    "This notebook implements the full machine learning workflow. It covers:\n",
    "1.  **Data Preparation**: Loading the final feature set and splitting it into training, validation, and holdout sets.\n",
    "2.  **Preprocessing**: Creating a robust pipeline to scale numerical features and one-hot encode categorical features.\n",
    "3.  **Model Tuning**: Training and tuning two separate XGBoost models optimized for different business goals (Precision and AUC-PR).\n",
    "4.  **Business Evaluation**: Using the tuned models on the holdout set to simulate a real-world, cost-sensitive fraud detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d58ca4",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e50ef",
   "metadata": {},
   "source": [
    "#### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edbd0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# --- Preprocessing & Modeling ---\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# --- Evaluation ---\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    brier_score_loss\n",
    ")\n",
    "from IPython.display import display # Added for display function\n",
    "\n",
    "# --- Model Persistence & Visualization ---\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress all warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca4cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Path Setup ---\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = Path(os.getcwd())\n",
    "\n",
    "# Navigate up one level to reach the project root directory\n",
    "project_root = notebook_dir.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import from config.py\n",
    "from bank_fraud.config import PROCESSED_DATA_DIR, REFERENCES_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbba70",
   "metadata": {},
   "source": [
    "### Load Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c4c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully from: data\\processed\\3.0_selected_features.parquet\n",
      "Dataset shape: (493189, 65)\n"
     ]
    }
   ],
   "source": [
    "# Load the final, curated dataset from the feature selection phase\n",
    "FINAL_DATA_PATH = PROCESSED_DATA_DIR / '3.0_selected_features.parquet'\n",
    "df = pd.read_parquet(FINAL_DATA_PATH)\n",
    "\n",
    "print(f\"Dataset loaded successfully from: {FINAL_DATA_PATH.relative_to(project_root)}\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb253a27",
   "metadata": {},
   "source": [
    "### Identify Feature Types and Define Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3786338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2 identifier columns: ['profile_id', 'account_no']\n",
      "Identified 59 numerical features.\n",
      "Identified 3 categorical features.\n"
     ]
    }
   ],
   "source": [
    "# --- Dynamically Drop Identifier Columns ---\n",
    "\n",
    "# Load the identifier data dictionary to get the authoritative list of identifiers\n",
    "IDENTIFIER_DICT_PATH = REFERENCES_DIR / 'identifier_data_dictionary.csv'\n",
    "identifier_df = pd.read_csv(IDENTIFIER_DICT_PATH)\n",
    "all_identifiers = identifier_df['feature_name'].tolist()\n",
    "\n",
    "# Find which of these identifiers are actually present in our current DataFrame\n",
    "# This ensures the script doesn't fail if a column was already dropped in a previous step.\n",
    "identifiers_to_drop = [col for col in all_identifiers if col in df.columns]\n",
    "\n",
    "# Define the target variable\n",
    "TARGET_COL = 'fraud_status'\n",
    "\n",
    "# Define the feature matrix X by dropping the target and all identified identifiers\n",
    "X = df.drop(columns=[TARGET_COL] + identifiers_to_drop, errors='ignore')\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(f\"Dropped {len(identifiers_to_drop)} identifier columns: {identifiers_to_drop}\")\n",
    "\n",
    "\n",
    "# Identify numerical and categorical features from the final feature matrix X\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Identified {len(numerical_features)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_features)} categorical features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ed50a",
   "metadata": {},
   "source": [
    "### Split Data into Training, Validation, and Holdout Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94bdbf",
   "metadata": {},
   "source": [
    "We will perform a stratified split to ensure the proportion of fraud cases is consistent across all datasets.\n",
    "- **Training Set (70%)**: For training the model.\n",
    "- **Validation Set (15%)**: For tuning hyperparameters.\n",
    "- **Holdout Set (15%)**: For final, unbiased evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ace76ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting complete.\n",
      "Training set shape:   (345232, 62)\n",
      "Validation set shape: (73978, 62)\n",
      "Holdout set shape:    (73979, 62)\n",
      "\n",
      "Proportion of fraud in each set:\n",
      "Training:   0.0166\n",
      "Validation: 0.0166\n",
      "Holdout:    0.0166\n"
     ]
    }
   ],
   "source": [
    "# First split: Create the training set (70%) and a temporary set (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.30, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Split the temporary set into validation (15%) and holdout (15%)\n",
    "# This is equivalent to splitting the 30% temp set in half (0.5)\n",
    "X_val, X_holdout, y_val, y_holdout = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.50, \n",
    "    random_state=42, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Data splitting complete.\")\n",
    "print(f\"Training set shape:   {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Holdout set shape:    {X_holdout.shape}\")\n",
    "print(\"\\nProportion of fraud in each set:\")\n",
    "print(f\"Training:   {y_train.mean():.4f}\")\n",
    "print(f\"Validation: {y_val.mean():.4f}\")\n",
    "print(f\"Holdout:    {y_holdout.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a001ec",
   "metadata": {},
   "source": [
    "### Establish Baseline with Proportion Chance Criterion (PCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475e2ca",
   "metadata": {},
   "source": [
    "Before building complex models, it's crucial to establish a baseline to understand the minimum performance we must exceed. For imbalanced classification tasks, simple accuracy can be misleading. The **Proportion Chance Criterion (PCC)** provides this baseline.\n",
    "\n",
    "The PCC represents the accuracy a naive model would achieve by always guessing the majority class. A common rule of thumb is that a useful model's accuracy should be at least 25% greater than the PCC.\n",
    "\n",
    "This calculation will demonstrate why we focus on metrics like Precision, Recall, and AUC-PR instead of accuracy alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93bd32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion Chance Criterion (PCC): 96.74%\n",
      "1.25 * PCC Threshold: 120.92%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Calculate PCC on the training data\n",
    "class_counts = Counter(y_train)\n",
    "total_samples = len(y_train)\n",
    "\n",
    "pcc = ((class_counts[0] / total_samples)**2) + ((class_counts[1] / total_samples)**2)\n",
    "pcc_threshold = 1.25 * pcc\n",
    "\n",
    "print(f\"Proportion Chance Criterion (PCC): {pcc:.2%}\")\n",
    "print(f\"1.25 * PCC Threshold: {pcc_threshold:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14897f37",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "The PCC of approximately 0.97 indicates that a model that does nothing but predict 'NON_FRAUD' for every case would be about 97% accurate. This high value underscores the inadequacy of accuracy as a primary metric for this problem. Our model must demonstrate a much more nuanced understanding of the data to be considered effective, which is why our evaluation will focus on its ability to correctly identify the rare fraud cases (Precision and Recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c8523",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3eb3ce0",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Pipeline Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f91cca",
   "metadata": {},
   "source": [
    "We will create a preprocessing pipeline using `ColumnTransformer` to apply different transformations to different types of columns.\n",
    "\n",
    "- **Numerical Features**: Will be scaled using `StandardScaler`. This standardizes features by removing the mean and scaling to unit variance, which is crucial for the performance of many machine learning algorithms.\n",
    "- **Categorical Features**: Will be transformed using `OneHotEncoder`. This converts categorical variables into a numerical format that can be provided to the model. `handle_unknown='ignore'` ensures that if a new category appears in the validation or holdout data (that was not seen in the training data), it will be handled gracefully without causing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1204eb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns (if any), though we expect none\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43359a33",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa36ca1",
   "metadata": {},
   "source": [
    "### 3.1. Baseline Model Comparison (No Resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163b984",
   "metadata": {},
   "source": [
    "We will first evaluate a set of baseline models without any resampling techniques to establish a performance benchmark.\n",
    "This step helps us understand the inherent performance of different algorithms on our imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f658d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "\n",
    "def auto_ml(X, y, models_dict, preprocessor, cv, res_t=None):\n",
    "    \"\"\"\n",
    "    Applies preprocessing, optional resampling, and evaluates multiple models using cross-validation.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    results_formatted = {}\n",
    "\n",
    "    for model_name, model_instance in models_dict.items():\n",
    "        print(f\"\\n--- Evaluating {model_name} ---\")\n",
    "        \n",
    "        # Create a pipeline that includes preprocessing and the model\n",
    "        if res_t is not None:\n",
    "            # If resampling is applied, use ImbPipeline\n",
    "            pipeline = ImbPipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('resampler', res_t),\n",
    "                ('classifier', model_instance)\n",
    "            ])\n",
    "        else:\n",
    "            # Otherwise, use standard Pipeline\n",
    "            pipeline = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', model_instance)\n",
    "            ])\n",
    "\n",
    "        train_ap, val_ap = [], []\n",
    "        train_bal_acc, val_bal_acc = [], []\n",
    "        train_f1_w, val_f1_w = [], []\n",
    "        train_mcc, val_mcc = [], []\n",
    "        train_brier, val_brier = [], []\n",
    "        train_precision, val_precision = [], []\n",
    "        train_recall, val_recall = [], []\n",
    "        \n",
    "        fold_times = []\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(cv.split(X, y)):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "            start_time = time.time()\n",
    "            pipeline.fit(X_train_fold, y_train_fold)\n",
    "            end_time = time.time()\n",
    "            fold_times.append(end_time - start_time)\n",
    "\n",
    "            # Predictions\n",
    "            train_preds = pipeline.predict(X_train_fold)\n",
    "            val_preds = pipeline.predict(X_val_fold)\n",
    "            \n",
    "            # Predict probabilities for metrics that require them\n",
    "            train_probas = pipeline.predict_proba(X_train_fold)[:, 1]\n",
    "            val_probas = pipeline.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "            # Calculate metrics\n",
    "            train_ap.append(average_precision_score(y_train_fold, train_probas))\n",
    "            val_ap.append(average_precision_score(y_val_fold, val_probas))\n",
    "\n",
    "            train_bal_acc.append(balanced_accuracy_score(y_train_fold, train_preds))\n",
    "            val_bal_acc.append(balanced_accuracy_score(y_val_fold, val_preds))\n",
    "\n",
    "            train_f1_w.append(f1_score(y_train_fold, train_preds, average='weighted'))\n",
    "            val_f1_w.append(f1_score(y_val_fold, val_preds, average='weighted'))\n",
    "\n",
    "            train_mcc.append(matthews_corrcoef(y_train_fold, train_preds))\n",
    "            val_mcc.append(matthews_corrcoef(y_val_fold, val_preds))\n",
    "            \n",
    "            train_brier.append(brier_score_loss(y_train_fold, train_probas))\n",
    "            val_brier.append(brier_score_loss(y_val_fold, val_probas))\n",
    "\n",
    "            train_precision.append(precision_score(y_train_fold, train_preds))\n",
    "            val_precision.append(precision_score(y_val_fold, val_preds))\n",
    "\n",
    "            train_recall.append(recall_score(y_train_fold, train_preds))\n",
    "            val_recall.append(recall_score(y_val_fold, val_preds))\n",
    "\n",
    "        # Store average results\n",
    "        results[model_name] = {\n",
    "            'Train AP': np.mean(train_ap),\n",
    "            'Val AP': np.mean(val_ap),\n",
    "            'Train BalAcc': np.mean(train_bal_acc),\n",
    "            'Val BalAcc': np.mean(val_bal_acc),\n",
    "            'Train F1_w': np.mean(train_f1_w),\n",
    "            'Val F1_w': np.mean(val_f1_w),\n",
    "            'Train MCC': np.mean(train_mcc),\n",
    "            'Val MCC': np.mean(val_mcc),\n",
    "            'Train Brier': np.mean(train_brier),\n",
    "            'Val Brier': np.mean(val_brier),\n",
    "            'Train Precision': np.mean(train_precision),\n",
    "            'Val Precision': np.mean(val_precision),\n",
    "            'Train Recall': np.mean(train_recall),\n",
    "            'Val Recall': np.mean(val_recall),\n",
    "            'Avg Run Time (s)': np.mean(fold_times)\n",
    "        }\n",
    "        \n",
    "        # Store formatted results\n",
    "        results_formatted[model_name] = {\n",
    "            'Train AP': f\"{np.mean(train_ap)*100:.2f}%\",\n",
    "            'Val AP': f\"{np.mean(val_ap)*100:.2f}%\",\n",
    "            'Train BalAcc': f\"{np.mean(train_bal_acc)*100:.2f}%\",\n",
    "            'Val BalAcc': f\"{np.mean(val_bal_acc)*100:.2f}%\",\n",
    "            'Train F1_w': f\"{np.mean(train_f1_w)*100:.2f}%\",\n",
    "            'Val F1_w': f\"{np.mean(val_f1_w)*100:.2f}%\",\n",
    "            'Train MCC': f\"{np.mean(train_mcc)*100:.2f}%\",\n",
    "            'Val MCC': f\"{np.mean(val_mcc)*100:.2f}%\",\n",
    "            'Train Brier': f\"{np.mean(train_brier)*100:.2f}%\",\n",
    "            'Val Brier': f\"{np.mean(val_brier)*100:.2f}%\",\n",
    "            'Train Precision': f\"{np.mean(train_precision)*100:.2f}%\",\n",
    "            'Val Precision': f\"{np.mean(val_precision)*100:.2f}%\",\n",
    "            'Train Recall': f\"{np.mean(train_recall)*100:.2f}%\",\n",
    "            'Val Recall': f\"{np.mean(val_recall)*100:.2f}%\",\n",
    "            'Avg Run Time (s)': f\"{np.mean(fold_times):.2f}\"\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(results).T, pd.DataFrame(results_formatted).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00045bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "# For XGBoost, scale_pos_weight is calculated based on the training data imbalance\n",
    "# eval_metric is set to 'logloss' for general classification, but AUC-PR is also tracked\n",
    "models_dict = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42, class_weight='balanced', max_depth=8, min_samples_leaf=20, ccp_alpha=0.001),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        scale_pos_weight=(len(y_train) - y_train.sum()) / y_train.sum(),\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss', # Use logloss for general evaluation, AUC-PR is tracked separately\n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a86450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating LogisticRegression ---\n",
      "\n",
      "--- Evaluating GaussianNB ---\n",
      "\n",
      "--- Evaluating DecisionTreeClassifier ---\n",
      "\n",
      "--- Evaluating XGBoost ---\n",
      "\n",
      "--- Baseline Model Performance (No Resampling) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train AP</th>\n",
       "      <th>Val AP</th>\n",
       "      <th>Train BalAcc</th>\n",
       "      <th>Val BalAcc</th>\n",
       "      <th>Train F1_w</th>\n",
       "      <th>Val F1_w</th>\n",
       "      <th>Train MCC</th>\n",
       "      <th>Val MCC</th>\n",
       "      <th>Train Brier</th>\n",
       "      <th>Val Brier</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Val Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Val Recall</th>\n",
       "      <th>Avg Run Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>54.72%</td>\n",
       "      <td>54.46%</td>\n",
       "      <td>79.89%</td>\n",
       "      <td>79.78%</td>\n",
       "      <td>96.69%</td>\n",
       "      <td>96.70%</td>\n",
       "      <td>35.84%</td>\n",
       "      <td>35.77%</td>\n",
       "      <td>9.94%</td>\n",
       "      <td>9.95%</td>\n",
       "      <td>22.13%</td>\n",
       "      <td>22.12%</td>\n",
       "      <td>63.55%</td>\n",
       "      <td>63.33%</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>20.19%</td>\n",
       "      <td>20.18%</td>\n",
       "      <td>75.51%</td>\n",
       "      <td>75.47%</td>\n",
       "      <td>97.37%</td>\n",
       "      <td>97.37%</td>\n",
       "      <td>36.90%</td>\n",
       "      <td>36.84%</td>\n",
       "      <td>3.09%</td>\n",
       "      <td>3.09%</td>\n",
       "      <td>27.49%</td>\n",
       "      <td>27.44%</td>\n",
       "      <td>53.40%</td>\n",
       "      <td>53.33%</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>27.36%</td>\n",
       "      <td>26.92%</td>\n",
       "      <td>78.88%</td>\n",
       "      <td>78.63%</td>\n",
       "      <td>96.69%</td>\n",
       "      <td>96.67%</td>\n",
       "      <td>35.07%</td>\n",
       "      <td>34.70%</td>\n",
       "      <td>10.69%</td>\n",
       "      <td>10.71%</td>\n",
       "      <td>22.03%</td>\n",
       "      <td>21.77%</td>\n",
       "      <td>61.50%</td>\n",
       "      <td>61.02%</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>69.00%</td>\n",
       "      <td>55.85%</td>\n",
       "      <td>84.36%</td>\n",
       "      <td>78.30%</td>\n",
       "      <td>98.62%</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>60.86%</td>\n",
       "      <td>50.16%</td>\n",
       "      <td>7.40%</td>\n",
       "      <td>7.70%</td>\n",
       "      <td>54.43%</td>\n",
       "      <td>45.18%</td>\n",
       "      <td>69.70%</td>\n",
       "      <td>57.78%</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Train AP  Val AP Train BalAcc Val BalAcc Train F1_w  \\\n",
       "LogisticRegression       54.72%  54.46%       79.89%     79.78%     96.69%   \n",
       "GaussianNB               20.19%  20.18%       75.51%     75.47%     97.37%   \n",
       "DecisionTreeClassifier   27.36%  26.92%       78.88%     78.63%     96.69%   \n",
       "XGBoost                  69.00%  55.85%       84.36%     78.30%     98.62%   \n",
       "\n",
       "                       Val F1_w Train MCC Val MCC Train Brier Val Brier  \\\n",
       "LogisticRegression       96.70%    35.84%  35.77%       9.94%     9.95%   \n",
       "GaussianNB               97.37%    36.90%  36.84%       3.09%     3.09%   \n",
       "DecisionTreeClassifier   96.67%    35.07%  34.70%      10.69%    10.71%   \n",
       "XGBoost                  98.25%    60.86%  50.16%       7.40%     7.70%   \n",
       "\n",
       "                       Train Precision Val Precision Train Recall Val Recall  \\\n",
       "LogisticRegression              22.13%        22.12%       63.55%     63.33%   \n",
       "GaussianNB                      27.49%        27.44%       53.40%     53.33%   \n",
       "DecisionTreeClassifier          22.03%        21.77%       61.50%     61.02%   \n",
       "XGBoost                         54.43%        45.18%       69.70%     57.78%   \n",
       "\n",
       "                       Avg Run Time (s)  \n",
       "LogisticRegression                 3.26  \n",
       "GaussianNB                         1.26  \n",
       "DecisionTreeClassifier             4.32  \n",
       "XGBoost                            3.32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Run baseline evaluation\n",
    "baseline_results, baseline_results_formatted = auto_ml(X_train, y_train, models_dict, preprocessor, cv)\n",
    "\n",
    "print(\"\\n--- Baseline Model Performance (No Resampling) ---\")\n",
    "display(baseline_results_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214b2a5",
   "metadata": {},
   "source": [
    "### 3.2. Resampling Techniques Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fe23b",
   "metadata": {},
   "source": [
    "Now, we will evaluate the impact of different resampling techniques on model performance.\n",
    "We will compare RandomOverSampler, SMOTE, and RandomUnderSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "458e784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating with RandomOverSampler ---\n",
      "\n",
      "--- Evaluating LogisticRegression ---\n",
      "\n",
      "--- Evaluating GaussianNB ---\n",
      "\n",
      "--- Evaluating DecisionTreeClassifier ---\n",
      "\n",
      "--- Evaluating XGBoost ---\n",
      "\n",
      "--- Evaluating with SMOTE ---\n",
      "\n",
      "--- Evaluating LogisticRegression ---\n",
      "\n",
      "--- Evaluating GaussianNB ---\n",
      "\n",
      "--- Evaluating DecisionTreeClassifier ---\n",
      "\n",
      "--- Evaluating XGBoost ---\n",
      "\n",
      "--- Evaluating with RandomUnderSampler ---\n",
      "\n",
      "--- Evaluating LogisticRegression ---\n",
      "\n",
      "--- Evaluating GaussianNB ---\n",
      "\n",
      "--- Evaluating DecisionTreeClassifier ---\n",
      "\n",
      "--- Evaluating XGBoost ---\n",
      "\n",
      "--- Model Performance with Resampling ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train AP</th>\n",
       "      <th>Val AP</th>\n",
       "      <th>Train BalAcc</th>\n",
       "      <th>Val BalAcc</th>\n",
       "      <th>Train F1_w</th>\n",
       "      <th>Val F1_w</th>\n",
       "      <th>Train MCC</th>\n",
       "      <th>Val MCC</th>\n",
       "      <th>Train Brier</th>\n",
       "      <th>Val Brier</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Val Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Val Recall</th>\n",
       "      <th>Avg Run Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression + RandomOverSampler</th>\n",
       "      <td>54.71%</td>\n",
       "      <td>54.44%</td>\n",
       "      <td>79.87%</td>\n",
       "      <td>79.75%</td>\n",
       "      <td>96.66%</td>\n",
       "      <td>96.66%</td>\n",
       "      <td>35.65%</td>\n",
       "      <td>35.54%</td>\n",
       "      <td>9.95%</td>\n",
       "      <td>9.95%</td>\n",
       "      <td>21.92%</td>\n",
       "      <td>21.87%</td>\n",
       "      <td>63.56%</td>\n",
       "      <td>63.33%</td>\n",
       "      <td>6.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB + RandomOverSampler</th>\n",
       "      <td>19.94%</td>\n",
       "      <td>19.91%</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>75.57%</td>\n",
       "      <td>97.32%</td>\n",
       "      <td>97.32%</td>\n",
       "      <td>36.54%</td>\n",
       "      <td>36.50%</td>\n",
       "      <td>3.17%</td>\n",
       "      <td>3.17%</td>\n",
       "      <td>26.87%</td>\n",
       "      <td>26.84%</td>\n",
       "      <td>53.68%</td>\n",
       "      <td>53.60%</td>\n",
       "      <td>2.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier + RandomOverSampler</th>\n",
       "      <td>27.67%</td>\n",
       "      <td>27.34%</td>\n",
       "      <td>78.85%</td>\n",
       "      <td>78.60%</td>\n",
       "      <td>96.77%</td>\n",
       "      <td>96.74%</td>\n",
       "      <td>35.54%</td>\n",
       "      <td>35.17%</td>\n",
       "      <td>10.67%</td>\n",
       "      <td>10.68%</td>\n",
       "      <td>22.63%</td>\n",
       "      <td>22.38%</td>\n",
       "      <td>61.30%</td>\n",
       "      <td>60.81%</td>\n",
       "      <td>9.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost + RandomOverSampler</th>\n",
       "      <td>67.67%</td>\n",
       "      <td>52.39%</td>\n",
       "      <td>64.12%</td>\n",
       "      <td>58.33%</td>\n",
       "      <td>43.40%</td>\n",
       "      <td>42.83%</td>\n",
       "      <td>8.06%</td>\n",
       "      <td>4.77%</td>\n",
       "      <td>60.80%</td>\n",
       "      <td>61.27%</td>\n",
       "      <td>2.30%</td>\n",
       "      <td>2.04%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>88.83%</td>\n",
       "      <td>6.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression + SMOTE</th>\n",
       "      <td>55.01%</td>\n",
       "      <td>54.75%</td>\n",
       "      <td>79.85%</td>\n",
       "      <td>79.75%</td>\n",
       "      <td>96.69%</td>\n",
       "      <td>96.70%</td>\n",
       "      <td>35.82%</td>\n",
       "      <td>35.75%</td>\n",
       "      <td>9.97%</td>\n",
       "      <td>9.98%</td>\n",
       "      <td>22.13%</td>\n",
       "      <td>22.12%</td>\n",
       "      <td>63.47%</td>\n",
       "      <td>63.26%</td>\n",
       "      <td>7.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB + SMOTE</th>\n",
       "      <td>20.01%</td>\n",
       "      <td>20.01%</td>\n",
       "      <td>75.71%</td>\n",
       "      <td>75.67%</td>\n",
       "      <td>97.32%</td>\n",
       "      <td>97.32%</td>\n",
       "      <td>36.59%</td>\n",
       "      <td>36.52%</td>\n",
       "      <td>3.18%</td>\n",
       "      <td>3.19%</td>\n",
       "      <td>26.82%</td>\n",
       "      <td>26.78%</td>\n",
       "      <td>53.91%</td>\n",
       "      <td>53.81%</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier + SMOTE</th>\n",
       "      <td>25.23%</td>\n",
       "      <td>25.20%</td>\n",
       "      <td>76.93%</td>\n",
       "      <td>76.92%</td>\n",
       "      <td>97.88%</td>\n",
       "      <td>97.88%</td>\n",
       "      <td>43.73%</td>\n",
       "      <td>43.66%</td>\n",
       "      <td>8.87%</td>\n",
       "      <td>8.88%</td>\n",
       "      <td>36.27%</td>\n",
       "      <td>36.17%</td>\n",
       "      <td>55.52%</td>\n",
       "      <td>55.49%</td>\n",
       "      <td>17.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost + SMOTE</th>\n",
       "      <td>56.58%</td>\n",
       "      <td>50.04%</td>\n",
       "      <td>64.47%</td>\n",
       "      <td>58.80%</td>\n",
       "      <td>44.77%</td>\n",
       "      <td>44.35%</td>\n",
       "      <td>8.15%</td>\n",
       "      <td>4.96%</td>\n",
       "      <td>59.21%</td>\n",
       "      <td>59.58%</td>\n",
       "      <td>2.32%</td>\n",
       "      <td>2.06%</td>\n",
       "      <td>99.53%</td>\n",
       "      <td>88.48%</td>\n",
       "      <td>7.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression + RandomUnderSampler</th>\n",
       "      <td>51.96%</td>\n",
       "      <td>51.57%</td>\n",
       "      <td>79.73%</td>\n",
       "      <td>79.64%</td>\n",
       "      <td>96.51%</td>\n",
       "      <td>96.51%</td>\n",
       "      <td>34.63%</td>\n",
       "      <td>34.56%</td>\n",
       "      <td>9.97%</td>\n",
       "      <td>9.98%</td>\n",
       "      <td>20.82%</td>\n",
       "      <td>20.81%</td>\n",
       "      <td>63.54%</td>\n",
       "      <td>63.36%</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB + RandomUnderSampler</th>\n",
       "      <td>18.23%</td>\n",
       "      <td>18.22%</td>\n",
       "      <td>75.99%</td>\n",
       "      <td>75.96%</td>\n",
       "      <td>97.06%</td>\n",
       "      <td>97.05%</td>\n",
       "      <td>34.83%</td>\n",
       "      <td>34.74%</td>\n",
       "      <td>3.61%</td>\n",
       "      <td>3.62%</td>\n",
       "      <td>24.16%</td>\n",
       "      <td>24.07%</td>\n",
       "      <td>54.91%</td>\n",
       "      <td>54.88%</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier + RandomUnderSampler</th>\n",
       "      <td>28.80%</td>\n",
       "      <td>28.44%</td>\n",
       "      <td>78.78%</td>\n",
       "      <td>78.59%</td>\n",
       "      <td>96.63%</td>\n",
       "      <td>96.60%</td>\n",
       "      <td>34.77%</td>\n",
       "      <td>34.39%</td>\n",
       "      <td>10.72%</td>\n",
       "      <td>10.74%</td>\n",
       "      <td>21.81%</td>\n",
       "      <td>21.49%</td>\n",
       "      <td>61.40%</td>\n",
       "      <td>61.04%</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost + RandomUnderSampler</th>\n",
       "      <td>59.57%</td>\n",
       "      <td>54.16%</td>\n",
       "      <td>58.84%</td>\n",
       "      <td>56.47%</td>\n",
       "      <td>29.60%</td>\n",
       "      <td>29.34%</td>\n",
       "      <td>5.96%</td>\n",
       "      <td>4.37%</td>\n",
       "      <td>68.43%</td>\n",
       "      <td>68.61%</td>\n",
       "      <td>2.01%</td>\n",
       "      <td>1.91%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>95.44%</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Train AP  Val AP Train BalAcc  \\\n",
       "LogisticRegression + RandomOverSampler        54.71%  54.44%       79.87%   \n",
       "GaussianNB + RandomOverSampler                19.94%  19.91%       75.61%   \n",
       "DecisionTreeClassifier + RandomOverSampler    27.67%  27.34%       78.85%   \n",
       "XGBoost + RandomOverSampler                   67.67%  52.39%       64.12%   \n",
       "LogisticRegression + SMOTE                    55.01%  54.75%       79.85%   \n",
       "GaussianNB + SMOTE                            20.01%  20.01%       75.71%   \n",
       "DecisionTreeClassifier + SMOTE                25.23%  25.20%       76.93%   \n",
       "XGBoost + SMOTE                               56.58%  50.04%       64.47%   \n",
       "LogisticRegression + RandomUnderSampler       51.96%  51.57%       79.73%   \n",
       "GaussianNB + RandomUnderSampler               18.23%  18.22%       75.99%   \n",
       "DecisionTreeClassifier + RandomUnderSampler   28.80%  28.44%       78.78%   \n",
       "XGBoost + RandomUnderSampler                  59.57%  54.16%       58.84%   \n",
       "\n",
       "                                            Val BalAcc Train F1_w Val F1_w  \\\n",
       "LogisticRegression + RandomOverSampler          79.75%     96.66%   96.66%   \n",
       "GaussianNB + RandomOverSampler                  75.57%     97.32%   97.32%   \n",
       "DecisionTreeClassifier + RandomOverSampler      78.60%     96.77%   96.74%   \n",
       "XGBoost + RandomOverSampler                     58.33%     43.40%   42.83%   \n",
       "LogisticRegression + SMOTE                      79.75%     96.69%   96.70%   \n",
       "GaussianNB + SMOTE                              75.67%     97.32%   97.32%   \n",
       "DecisionTreeClassifier + SMOTE                  76.92%     97.88%   97.88%   \n",
       "XGBoost + SMOTE                                 58.80%     44.77%   44.35%   \n",
       "LogisticRegression + RandomUnderSampler         79.64%     96.51%   96.51%   \n",
       "GaussianNB + RandomUnderSampler                 75.96%     97.06%   97.05%   \n",
       "DecisionTreeClassifier + RandomUnderSampler     78.59%     96.63%   96.60%   \n",
       "XGBoost + RandomUnderSampler                    56.47%     29.60%   29.34%   \n",
       "\n",
       "                                            Train MCC Val MCC Train Brier  \\\n",
       "LogisticRegression + RandomOverSampler         35.65%  35.54%       9.95%   \n",
       "GaussianNB + RandomOverSampler                 36.54%  36.50%       3.17%   \n",
       "DecisionTreeClassifier + RandomOverSampler     35.54%  35.17%      10.67%   \n",
       "XGBoost + RandomOverSampler                     8.06%   4.77%      60.80%   \n",
       "LogisticRegression + SMOTE                     35.82%  35.75%       9.97%   \n",
       "GaussianNB + SMOTE                             36.59%  36.52%       3.18%   \n",
       "DecisionTreeClassifier + SMOTE                 43.73%  43.66%       8.87%   \n",
       "XGBoost + SMOTE                                 8.15%   4.96%      59.21%   \n",
       "LogisticRegression + RandomUnderSampler        34.63%  34.56%       9.97%   \n",
       "GaussianNB + RandomUnderSampler                34.83%  34.74%       3.61%   \n",
       "DecisionTreeClassifier + RandomUnderSampler    34.77%  34.39%      10.72%   \n",
       "XGBoost + RandomUnderSampler                    5.96%   4.37%      68.43%   \n",
       "\n",
       "                                            Val Brier Train Precision  \\\n",
       "LogisticRegression + RandomOverSampler          9.95%          21.92%   \n",
       "GaussianNB + RandomOverSampler                  3.17%          26.87%   \n",
       "DecisionTreeClassifier + RandomOverSampler     10.68%          22.63%   \n",
       "XGBoost + RandomOverSampler                    61.27%           2.30%   \n",
       "LogisticRegression + SMOTE                      9.98%          22.13%   \n",
       "GaussianNB + SMOTE                              3.19%          26.82%   \n",
       "DecisionTreeClassifier + SMOTE                  8.88%          36.27%   \n",
       "XGBoost + SMOTE                                59.58%           2.32%   \n",
       "LogisticRegression + RandomUnderSampler         9.98%          20.82%   \n",
       "GaussianNB + RandomUnderSampler                 3.62%          24.16%   \n",
       "DecisionTreeClassifier + RandomUnderSampler    10.74%          21.81%   \n",
       "XGBoost + RandomUnderSampler                   68.61%           2.01%   \n",
       "\n",
       "                                            Val Precision Train Recall  \\\n",
       "LogisticRegression + RandomOverSampler             21.87%       63.56%   \n",
       "GaussianNB + RandomOverSampler                     26.84%       53.68%   \n",
       "DecisionTreeClassifier + RandomOverSampler         22.38%       61.30%   \n",
       "XGBoost + RandomOverSampler                         2.04%      100.00%   \n",
       "LogisticRegression + SMOTE                         22.12%       63.47%   \n",
       "GaussianNB + SMOTE                                 26.78%       53.91%   \n",
       "DecisionTreeClassifier + SMOTE                     36.17%       55.52%   \n",
       "XGBoost + SMOTE                                     2.06%       99.53%   \n",
       "LogisticRegression + RandomUnderSampler            20.81%       63.54%   \n",
       "GaussianNB + RandomUnderSampler                    24.07%       54.91%   \n",
       "DecisionTreeClassifier + RandomUnderSampler        21.49%       61.40%   \n",
       "XGBoost + RandomUnderSampler                        1.91%      100.00%   \n",
       "\n",
       "                                            Val Recall Avg Run Time (s)  \n",
       "LogisticRegression + RandomOverSampler          63.33%             6.52  \n",
       "GaussianNB + RandomOverSampler                  53.60%             2.15  \n",
       "DecisionTreeClassifier + RandomOverSampler      60.81%             9.18  \n",
       "XGBoost + RandomOverSampler                     88.83%             6.63  \n",
       "LogisticRegression + SMOTE                      63.26%             7.41  \n",
       "GaussianNB + SMOTE                              53.81%             2.64  \n",
       "DecisionTreeClassifier + SMOTE                  55.49%            17.86  \n",
       "XGBoost + SMOTE                                 88.48%             7.85  \n",
       "LogisticRegression + RandomUnderSampler         63.36%             0.97  \n",
       "GaussianNB + RandomUnderSampler                 54.88%             0.91  \n",
       "DecisionTreeClassifier + RandomUnderSampler     61.04%             1.00  \n",
       "XGBoost + RandomUnderSampler                    95.44%             1.37  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resamplers_dict = {\n",
    "    'RandomOverSampler': RandomOverSampler(random_state=42),\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'RandomUnderSampler': RandomUnderSampler(random_state=42)\n",
    "}\n",
    "\n",
    "resampling_results = {}\n",
    "resampling_results_formatted = {}\n",
    "\n",
    "for resampler_name, resampler_instance in resamplers_dict.items():\n",
    "    print(f\"\\n--- Evaluating with {resampler_name} ---\")\n",
    "    # For each resampler, evaluate all models\n",
    "    current_res_results, current_res_results_formatted = auto_ml(X_train, y_train, models_dict, preprocessor, cv, res_t=resampler_instance)\n",
    "    \n",
    "    # Store results, potentially renaming columns to indicate resampler\n",
    "    for model_name in current_res_results.index:\n",
    "        # Use a combined key for model and resampler\n",
    "        combined_key = f\"{model_name} + {resampler_name}\"\n",
    "        resampling_results[combined_key] = current_res_results.loc[model_name].to_dict()\n",
    "        resampling_results_formatted[combined_key] = current_res_results_formatted.loc[model_name].to_dict()\n",
    "\n",
    "resampling_results_df = pd.DataFrame(resampling_results).T\n",
    "resampling_results_formatted_df = pd.DataFrame(resampling_results_formatted).T\n",
    "\n",
    "print(\"\\n--- Model Performance with Resampling ---\")\n",
    "display(resampling_results_formatted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf98b9f",
   "metadata": {},
   "source": [
    "### 3.3. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3653e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model performance saved to: reports\\model_evaluation\\baseline_model_performance.csv\n",
      "Resampling model performance saved to: reports\\model_evaluation\\resampling_model_performance.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the directory for saving model evaluation results\n",
    "MODEL_EVAL_DIR = project_root / 'reports' / 'model_evaluation'\n",
    "MODEL_EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save baseline results\n",
    "baseline_results_formatted.to_csv(MODEL_EVAL_DIR / 'baseline_model_performance.csv', index=True)\n",
    "print(f\"Baseline model performance saved to: {MODEL_EVAL_DIR.relative_to(project_root) / 'baseline_model_performance.csv'}\")\n",
    "\n",
    "# Save resampling results\n",
    "resampling_results_formatted_df.to_csv(MODEL_EVAL_DIR / 'resampling_model_performance.csv', index=True)\n",
    "print(f\"Resampling model performance saved to: {MODEL_EVAL_DIR.relative_to(project_root) / 'resampling_model_performance.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855e6ef",
   "metadata": {},
   "source": [
    "### 3.4. Decision on Resampling and Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1a9f0",
   "metadata": {},
   "source": [
    "Based on the comparison of baseline models and models with resampling, we have made the following observations and decisions:\n",
    "\n",
    "**Analysis of Baseline Models (No Resampling):**\n",
    "- **XGBoost** demonstrated superior performance across key metrics, particularly `Val AP` (55.85%) and `Val Precision` (45.18%), compared to Logistic Regression, GaussianNB, and Decision Tree Classifier. This indicates its strong ability to identify fraud cases while maintaining a reasonable false positive rate without explicit external resampling.\n",
    "\n",
    "**Analysis of Resampling Techniques:**\n",
    "- For **XGBoost**, applying external resampling techniques (`RandomOverSampler`, `SMOTE`, `RandomUnderSampler`) generally led to a significant increase in `Val Recall` (e.g., up to 95.44% with RandomUnderSampler).\n",
    "- However, this increase in recall came at a substantial cost to `Val Precision` (dropping to as low as 1.91%) and `Val AP` (dropping to around 50-54%). This trade-off suggests that while more fraud cases are identified, a much higher number of non-fraud cases are also flagged as fraudulent, which is undesirable for the \"Auto-Blocking Leg\" that prioritizes minimizing false positives.\n",
    "- For other models (Logistic Regression, GaussianNB, Decision Tree), resampling did not consistently provide significant improvements in `Val AP` or `Val Precision` that would justify their use over XGBoost.\n",
    "\n",
    "**Decision on Resampling and Final Model Selection:**\n",
    "- We will proceed with **XGBoost without an explicit external resampling technique** for hyperparameter tuning. The `scale_pos_weight` parameter within XGBoost itself is already effectively handling the class imbalance by giving more importance to the minority class during training. This approach has shown the best balance between Precision and AUC-PR in our initial evaluations.\n",
    "\n",
    "Our primary optimization targets for tuning will remain:\n",
    "- **Precision**: Crucial for the \"Auto-Blocking Leg\" to minimize false positives.\n",
    "- **Average Precision (AUC-PR)**: A robust metric for overall ranking quality in imbalanced datasets, important for both legs of the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b92b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review the tables above and the saved CSV files to compare model performance with and without resampling.\n",
      "Based on Precision and AUC-PR, decide which combination (model + optional resampler) to proceed with for hyperparameter tuning.\n",
      "This decision will inform the next steps in the notebook.\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for decision and next steps\n",
    "print(\"\\nReview the tables above and the saved CSV files to compare model performance with and without resampling.\")\n",
    "print(\"Based on Precision and AUC-PR, decide which combination (model + optional resampler) to proceed with for hyperparameter tuning.\")\n",
    "print(\"This decision will inform the next steps in the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99aa65",
   "metadata": {},
   "source": [
    "### 3.5. Hyperparameter Tuning for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b6869f",
   "metadata": {},
   "source": [
    "We will use a two-stage hyperparameter tuning process:\n",
    "1.  **Stage 1: RandomizedSearchCV**: To efficiently explore a broad range of hyperparameters and identify promising regions.\n",
    "2.  **Stage 2: GridSearchCV**: To perform a more exhaustive search within the narrowed, promising regions found by RandomizedSearchCV.\n",
    "\n",
    "We will perform this two-stage tuning for two separate objectives: one optimized for Precision and another for AUC-PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ced2ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 1: Randomized Search for Precision ---\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "Best parameters from Randomized Search (Precision):\n",
      "{'classifier__subsample': 1.0, 'classifier__reg_lambda': 0, 'classifier__reg_alpha': 0, 'classifier__n_estimators': 500, 'classifier__max_depth': 9, 'classifier__learning_rate': 0.1, 'classifier__gamma': 0.3, 'classifier__colsample_bytree': 1.0}\n",
      "\n",
      "Best Precision score from Randomized Search:\n",
      "0.4849348517547364\n",
      "\n",
      "--- Stage 1: Randomized Search for AUC-PR ---\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "Best parameters from Randomized Search (AUC-PR):\n",
      "{'classifier__subsample': 1.0, 'classifier__reg_lambda': 0.001, 'classifier__reg_alpha': 0.01, 'classifier__n_estimators': 500, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.01, 'classifier__gamma': 0, 'classifier__colsample_bytree': 0.6}\n",
      "\n",
      "Best AUC-PR score from Randomized Search:\n",
      "0.5780963006177268\n"
     ]
    }
   ],
   "source": [
    "# Define the XGBoost model instance (without external resampler, as decided)\n",
    "# The scale_pos_weight is already set in the models_dict for XGBoost\n",
    "xgb_model_for_tuning = models_dict['XGBoost']\n",
    "\n",
    "# Create a pipeline for tuning that includes the preprocessor and the XGBoost model\n",
    "pipeline_for_tuning = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb_model_for_tuning)\n",
    "])\n",
    "\n",
    "# --- Stage 1: RandomizedSearchCV ---\n",
    "# Define a broader parameter distribution for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 5, 7, 9],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'classifier__reg_alpha': [0, 0.001, 0.01, 0.1], # L1 regularization\n",
    "    'classifier__reg_lambda': [0, 0.001, 0.01, 0.1] # L2 regularization\n",
    "}\n",
    "\n",
    "# --- Randomized Search for Precision ---\n",
    "print(\"\\n--- Stage 1: Randomized Search for Precision ---\")\n",
    "random_search_precision = RandomizedSearchCV(\n",
    "    estimator=pipeline_for_tuning,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    scoring='precision',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search_precision.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters from Randomized Search (Precision):\")\n",
    "print(random_search_precision.best_params_)\n",
    "print(\"\\nBest Precision score from Randomized Search:\")\n",
    "print(random_search_precision.best_score_)\n",
    "\n",
    "# --- Randomized Search for AUC-PR ---\n",
    "print(\"\\n--- Stage 1: Randomized Search for AUC-PR ---\")\n",
    "random_search_aucpr = RandomizedSearchCV(\n",
    "    estimator=pipeline_for_tuning,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring='average_precision',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search_aucpr.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters from Randomized Search (AUC-PR):\")\n",
    "print(random_search_aucpr.best_params_)\n",
    "print(\"\\nBest AUC-PR score from Randomized Search:\")\n",
    "print(random_search_aucpr.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3f5e0",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef44e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Search (Precision) results saved to: reports\\model_evaluation\\random_search_precision_results.csv\n",
      "Randomized Search (AUC-PR) results saved to: reports\\model_evaluation\\random_search_aucpr_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save RandomizedSearchCV results for Precision\n",
    "pd.DataFrame(random_search_precision.cv_results_).to_csv(MODEL_EVAL_DIR / 'random_search_precision_results.csv', index=False)\n",
    "\n",
    "print(f\"Randomized Search (Precision) results saved to: {MODEL_EVAL_DIR.relative_to(project_root) / 'random_search_precision_results.csv'}\")\n",
    "\n",
    "# Save RandomizedSearchCV results for AUC-PR\n",
    "pd.DataFrame(random_search_aucpr.cv_results_).to_csv(MODEL_EVAL_DIR / 'random_search_aucpr_results.csv', index=False)\n",
    "\n",
    "print(f\"Randomized Search (AUC-PR) results saved to: {MODEL_EVAL_DIR.relative_to(project_root) / 'random_search_aucpr_results.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
